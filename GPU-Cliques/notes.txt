--- RESEARCH NOVELTIES ---

0 - PRUNING RULES - Adapted from the Quick algorithm. Lookahead, diameter, degree, lower-upper, critical vertex

1 - TASK-BASED DESIGN - bfs rather than dfs allows for independant tasks.

2 - MEMORY BOUNDING STRATEGY - To address the problem of the size of the tasks array-based data structure, the number of tasks expanded at each level will be limited, instead 
    expanding certain ones until they are complete then moving onto others. Since each task can generate thousands of others there will be no lack of tasks to parallelize even 
    with limited expansion.

3 - ARRAY-BASED DATA-STRUCTURE - tasks1, tasks2, and the buffer all use multiple arrays to represent the vertex-task data. tasks1 and 2 are small in size as they transfer all 
    data to each other each level. The buffer is large in size as it acts to fill these, remaining inactive otherwise. Allows for coalesced memory access.

4 - BUFFER STACK - The buffer acts as a stack since it is used for push and pop operations. When a level generates to many tasks the excess are pushed onto the top of the the 
    stack. When a level does not generate enough tasks the tasks arrays are filled from poping tasks from the top of the buffer.

5 - AVOIDED DUPLICATE CALCULATIONS - unlike the (GPU Clique Mining?) algo we did not have to perform expansion twice, once to calculate write positions, next to get data. 
    This is done by utilizing our global-memory warp-buffer data-structures

6 - DYNAMIC WARP LEVEL MEMORY SELECTION - warp will store vertices in either shared or global memory depending on its size

7 - PRUNING INTERSECTION SPEED IMPROVEMENT - using the second intersection technique of for all vert, for all removed vert, binary search neighbors(rvert), we speed up 
    intersection technique by improving the the neighbors(rvert) aspect from a linear factor to a log factor

8 - GPU ORIENTATED DESIGN - the work is devided at the warp level, with each warp handled its own, independant task, with 32 threads assigned to it. This allows for
    effective parallel solving of the problem.

10 - HYBRID CPU / GPU EXPANSION - Program has exact control of number of tasks that will be handled on the CPU. As the early tasks can be very computationally expensive
     performing the first round of pruning or even just the first few tasks on the CPU can significantly decerase the memory usage on the GPU.
    


--- REMOVED VARIABLES ---

int idx = (blockIdx.x * blockDim.x + threadIdx.x);

int warp_idx = (idx / WARP_SIZE);
int lane_idx = (idx % WARP_SIZE);
const int warps_per_block = (BLOCK_SIZE / WARP_SIZE);
int warp_in_block_idx = ((idx / WARP_SIZE) % (BLOCK_SIZE / WARP_SIZE));

// warp buffer write starts
int wtasks_write = (WTASKS_SIZE * (idx / WARP_SIZE));
int wtasks_offset_write = (WTASKS_OFFSET_SIZE * (idx / WARP_SIZE));
int wcliques_write = (WCLIQUES_SIZE * (idx / WARP_SIZE));
int wcliques_offset_write = (WCLIQUES_OFFSET_SIZE * (idx / WARP_SIZE));
int wvertices_write = (WVERTICES_SIZE * (idx / WARP_SIZE));
int svertices_write = (VERTICES_SIZE * ((idx / WARP_SIZE) % (BLOCK_SIZE / WARP_SIZE));

--- FROM DIAMETER PRUNING ---
    // scan to calculate write postion in warp arrays
    phelper2 = lane_remaining_count;
    for (int i = 1; i < WARP_SIZE; i *= 2) {
        phelper1 = __shfl_up_sync(0xFFFFFFFF, lane_remaining_count, i, WARP_SIZE);
        if ((ld.idx % WARP_SIZE) >= i) {
            lane_remaining_count += phelper1;
        }
        __syncwarp();
    }
    // lane remaining count sum is scan for last lane and its value
    if ((ld.idx % WARP_SIZE) == WARP_SIZE - 1) {
        wd.remaining_count[ld.wib_idx] = lane_remaining_count;
    }
    // make scan exclusive
    lane_remaining_count -= phelper2;

    // parallel write lane arrays to warp array
    for (int i = 0; i < phelper2; i++) {
        dd.remaining_candidates[(WVERTICES_SIZE * (ld.idx / WARP_SIZE)) + lane_remaining_count + i] = ld.vertices[dd.lane_remaining_candidates[lane_write + i]];
        dd.candidate_indegs[(WVERTICES_SIZE * (ld.idx / WARP_SIZE)) + lane_remaining_count + i] = dd.lane_candidate_indegs[lane_write + i];
    }
    __syncwarp();

    // DEBUG
    if (ld.idx == 0) {
        for (int i = 0; i < wd.remaining_count[ld.wib_idx]; i++) {
            //printf("%i ", dd.candidate_indegs[(WVERTICES_SIZE * (ld.idx / WARP_SIZE)) + i]);
        }
        //printf("\n\n");
    }



    // reset exdegs
    for (int i = (ld.idx % WARP_SIZE); i < wd.number_of_members[ld.wib_idx]; i += WARP_SIZE) {
        ld.vertices[i].exdeg = 0;
    }
    for (int i = (ld.idx % WARP_SIZE); i < wd.remaining_count[ld.wib_idx]; i += WARP_SIZE) {
        dd.remaining_candidates[(WVERTICES_SIZE * (ld.idx / WARP_SIZE)) + i].exdeg = 0;
    }
    __syncwarp();

    // update exdeg based on remaining candidates
    for (int i = (ld.idx % WARP_SIZE); i < wd.number_of_members[ld.wib_idx]; i += WARP_SIZE) {
        pvertexid = ld.vertices[i].vertexid;

        for (int j = 0; j < wd.remaining_count[ld.wib_idx]; j++) {
            phelper1 = dd.remaining_candidates[(WVERTICES_SIZE * (ld.idx / WARP_SIZE)) + j].vertexid;
            phelper2 = d_bsearch_array(dd.onehop_neighbors + dd.onehop_offsets[phelper1], dd.onehop_offsets[phelper1 + 1] - dd.onehop_offsets[phelper1], pvertexid);

            if (phelper2 > -1) {
                ld.vertices[i].exdeg++;
            }
        }
    }
    for (int i = (ld.idx % WARP_SIZE); i < wd.remaining_count[ld.wib_idx]; i += WARP_SIZE) {
        pvertexid = dd.remaining_candidates[(WVERTICES_SIZE * (ld.idx / WARP_SIZE)) + i].vertexid;

        for (int j = 0; j < wd.remaining_count[ld.wib_idx]; j++) {
            if (j == i) {
                continue;
            }

            phelper1 = dd.remaining_candidates[(WVERTICES_SIZE * (ld.idx / WARP_SIZE)) + j].vertexid;
            phelper2 = d_bsearch_array(dd.onehop_neighbors + dd.onehop_offsets[phelper1], dd.onehop_offsets[phelper1 + 1] - dd.onehop_offsets[phelper1], pvertexid);

            if (phelper2 > -1) {
                dd.remaining_candidates[(WVERTICES_SIZE * (ld.idx / WARP_SIZE)) + i].exdeg++;
            }
        }
    }
    __syncwarp();



    // condense vertices so remaining are after members
    for (int i = (ld.idx % WARP_SIZE); i < wd.remaining_count[ld.wib_idx]; i += WARP_SIZE) {
        ld.vertices[wd.number_of_members[ld.wib_idx] + i] = dd.remaining_candidates[(WVERTICES_SIZE * (ld.idx / WARP_SIZE)) + i];
    }

    if ((ld.idx % WARP_SIZE) == 0) {
        wd.total_vertices[ld.wib_idx] = wd.total_vertices[ld.wib_idx] - wd.number_of_candidates[ld.wib_idx] + wd.remaining_count[ld.wib_idx];
        wd.number_of_candidates[ld.wib_idx] = wd.remaining_count[ld.wib_idx];
    }